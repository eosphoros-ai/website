# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-08-21 16:59+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/deploy/deploy.md:1
#: 14020ee624b545a5a034b7e357f42545
msgid "Installation From Source"
msgstr "源码安装"

#: ../../getting_started/install/deploy/deploy.md:3
#: eeafb53bf0e846518457084d84edece7
msgid ""
"This tutorial gives you a quick walkthrough about use DB-GPT with you "
"environment and data."
msgstr "本教程为您提供了关于如何使用DB-GPT的使用指南。"

#: ../../getting_started/install/deploy/deploy.md:5
#: 1d6ee2f0f1ae43e9904da4c710b13e28
msgid "Installation"
msgstr "安装"

#: ../../getting_started/install/deploy/deploy.md:7
#: 6ebdb4ae390e4077af2388c48a73430d
msgid "To get started, install DB-GPT with the following steps."
msgstr "请按照以下步骤安装DB-GPT"

#: ../../getting_started/install/deploy/deploy.md:9
#: 910cfe79d1064bd191d56957b76d37fa
msgid "1. Hardware Requirements"
msgstr "1. 硬件要求"

#: ../../getting_started/install/deploy/deploy.md:10
#: 6207b8e32b7c4b669c8874ff9267627e
msgid ""
"As our project has the ability to achieve ChatGPT performance of over "
"85%, there are certain hardware requirements. However, overall, the "
"project can be deployed and used on consumer-grade graphics cards. The "
"specific hardware requirements for deployment are as follows:"
msgstr "由于我们的项目有能力达到85%以上的ChatGPT性能，所以对硬件有一定的要求。但总体来说，我们在消费级的显卡上即可完成项目的部署使用，具体部署的硬件说明如下:"

#: ../../getting_started/install/deploy/deploy.md
#: 45babe2e028746559e437880fdbcd5d3
msgid "GPU"
msgstr "GPU"

#: ../../getting_started/install/deploy/deploy.md
#: 7adbc2bb5e384d419b53badfaf36b962 9307790f5c464f58a54c94659451a037
msgid "VRAM Size"
msgstr "显存"

#: ../../getting_started/install/deploy/deploy.md
#: 305fdfdbd4674648a059b65736be191c
msgid "Performance"
msgstr "Performance"

#: ../../getting_started/install/deploy/deploy.md
#: 0e719a22b08844d9be04b4bcaeb4ad87
msgid "RTX 4090"
msgstr "RTX 4090"

#: ../../getting_started/install/deploy/deploy.md
#: 482dd0da73f3495198ee1c9c8fb7e8ed ed30edd1a6944d6c8cb6a06c9c12d4db
msgid "24 GB"
msgstr "24 GB"

#: ../../getting_started/install/deploy/deploy.md
#: b9a9b3179d844b97a578193eacfec8cc
msgid "Smooth conversation inference"
msgstr "Smooth conversation inference"

#: ../../getting_started/install/deploy/deploy.md
#: 171da7c9f0744b5aa335a5411f126eb7
msgid "RTX 3090"
msgstr "RTX 3090"

#: ../../getting_started/install/deploy/deploy.md
#: fbb497f41e61437ba089008c573b0cc7
msgid "Smooth conversation inference, better than V100"
msgstr "Smooth conversation inference, better than V100"

#: ../../getting_started/install/deploy/deploy.md
#: 2cb4fba16b664e1e9c22a1076f837a80
msgid "V100"
msgstr "V100"

#: ../../getting_started/install/deploy/deploy.md
#: 05cccda43ffb41d7b73c2d5dfbc7f1c5 8c471150ab0746d8998ddca30ad86404
msgid "16 GB"
msgstr "16 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 335fa32f77a349abb9a813a3a9dd6974 8546d09080b0421597540e92ab485254
msgid "Conversation inference possible, noticeable stutter"
msgstr "Conversation inference possible, noticeable stutter"

#: ../../getting_started/install/deploy/deploy.md
#: dd7a18f4bc144413b90863598c5e9a83
msgid "T4"
msgstr "T4"

#: ../../getting_started/install/deploy/deploy.md:19
#: 932dc4db2fba4272b72c31eb7d319255
msgid ""
"if your VRAM Size is not enough, DB-GPT supported 8-bit quantization and "
"4-bit quantization."
msgstr "如果你的显存不够，DB-GPT支持8-bit和4-bit量化版本"

#: ../../getting_started/install/deploy/deploy.md:21
#: 7dd6fabaf1ea43718f26e8b83a7299e3
msgid ""
"Here are some of the VRAM size usage of the models we tested in some "
"common scenarios."
msgstr "这里是量化版本的相关说明"

#: ../../getting_started/install/deploy/deploy.md
#: b50ded065d4943e3a5bfdfdf3a723f82
msgid "Model"
msgstr "Model"

#: ../../getting_started/install/deploy/deploy.md
#: 30621cf2407f4beca262eb47023d0b84
msgid "Quantize"
msgstr "Quantize"

#: ../../getting_started/install/deploy/deploy.md
#: 492112b927ce46308c50917beaa9e23a 8450f0b95a05475d9136906ec64d43b2
msgid "vicuna-7b-v1.5"
msgstr "vicuna-7b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 1379c29cb10340848ce3e9bf9ec67348 39221fd99f0a41d29141fb73e1c9217d
#: 3fd499ced4884e2aa6633784432f085c 6e872d37f2ab4571961465972092f439
#: 917c7d492a4943f5963a210f9c997cb7 fb204224fb484019b344315d03f50571
#: ff9f5aea13d04176912ebf141cc15d44
msgid "4-bit"
msgstr "4-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 13976c780bc3451fae4ad398b39f5245 32fe2b6c2f1e40c8928c8537f9239d07
#: 74f4ff229a314abf97c8fa4d6d73c339
msgid "8 GB"
msgstr "8 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 14ca9edebe794c7692738e5d14a15c06 453394769a20437da7a7a758c35345af
#: 860b51d1ab6742c5bdefc6e1ffc923a3 988110ac5e7f418d8a46ea9c42238ecc
#: d446bc441f8946d9a95a2a965157478e d683ae90e3da473f876cb948dd5dce5e
#: f1e2f6fb36624185ab2876bdc87301ec
msgid "8-bit"
msgstr "8-bit"

#: ../../getting_started/install/deploy/deploy.md
#: 1b6230690780434184bab3a68d501b60 734f124ce767437097d9cb3584796df5
#: 7da1c33a6d4746eba7f976cc43e6ad59 c0e7bd4672014afe88be1e11b8e772da
#: d045ee257f8a40e8bdad0e2b91c64018 e231e0b6f7ee4f5b85d28a18bbc32175
msgid "12 GB"
msgstr "12 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 2ee16dd760e74439bbbec13186f9e44e 65628a09b7bf487eb4b4f0268eab2751
msgid "vicuna-13b-v1.5"
msgstr "vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md
#: 3d633ad2c90547d1b47a06aafe4aa177 4a9e81e6303748ada67fa8b6ec1a8f57
#: d33dc735503649348590e03efabef94d
msgid "20 GB"
msgstr "20 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 6c02020d52e94f27b2eb1d10b20e9cca d58f181d10c04980b6d0bdc2be51c01c
msgid "llama-2-7b"
msgstr "llama-2-7b"

#: ../../getting_started/install/deploy/deploy.md
#: b9f25fa66dc04e539dfa30b2297ac8aa e1e80256f5444e9aacebb64e053a6b70
msgid "llama-2-13b"
msgstr "llama-2-13b"

#: ../../getting_started/install/deploy/deploy.md
#: 077c1d5b6db248d4aa6a3b0c5b2cc237 987cc7f04eea4d94acd7ca3ee0fdfe20
msgid "llama-2-70b"
msgstr "llama-2-70b"

#: ../../getting_started/install/deploy/deploy.md
#: 18a1070ab2e048c7b3ea90e25d58b38f
msgid "48 GB"
msgstr "48 GB"

#: ../../getting_started/install/deploy/deploy.md
#: da0474d2c8214e678021181191a651e5
msgid "80 GB"
msgstr "80 GB"

#: ../../getting_started/install/deploy/deploy.md
#: 49e5fedf491b4b569459c23e4f6ebd69 8b54730a306a46f7a531c13ef825b20a
msgid "baichuan-7b"
msgstr "baichuan-7b"

#: ../../getting_started/install/deploy/deploy.md
#: 2158255ea61c4a0c9e96bec0df21fa06 694d9b2fe90740c5b9704089d7ddac9d
msgid "baichuan-13b"
msgstr "baichuan-13b"

#: ../../getting_started/install/deploy/deploy.md:40
#: a9f9e470d41f4122a95cbc8bd2bc26dc
msgid "2. Install"
msgstr "2. Install"

#: ../../getting_started/install/deploy/deploy.md:45
#: c78fd491a6374224ab95dc39849f871f
msgid ""
"We use Sqlite as default database, so there is no need for database "
"installation.  If you choose to connect to other databases, you can "
"follow our tutorial for installation and configuration.  For the entire "
"installation process of DB-GPT, we use the miniconda3 virtual "
"environment. Create a virtual environment and install the Python "
"dependencies. [How to install "
"Miniconda](https://docs.conda.io/en/latest/miniconda.html)"
msgstr ""
"目前使用Sqlite作为默认数据库，因此DB-"
"GPT快速部署不需要部署相关数据库服务。如果你想使用其他数据库，需要先部署相关数据库服务。我们目前使用Miniconda进行python环境和包依赖管理[安装"
" Miniconda](https://docs.conda.io/en/latest/miniconda.html)"

#: ../../getting_started/install/deploy/deploy.md:54
#: 12180cd023a04152b1591a87d96d227a
msgid "Before use DB-GPT Knowledge"
msgstr "在使用知识库之前"

#: ../../getting_started/install/deploy/deploy.md:60
#: 2d5c1e241a0b47de81c91eca2c4999c6
msgid ""
"Once the environment is installed, we have to create a new folder "
"\"models\" in the DB-GPT project, and then we can put all the models "
"downloaded from huggingface in this directory"
msgstr "如果你已经安装好了环境需要创建models, 然后到huggingface官网下载模型"

#: ../../getting_started/install/deploy/deploy.md:63
#: 8a79092303e74ab2974fb6edd3d14a1c
msgid "Notice make sure you have install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:65
#: 68e9b02da9994192856fd4572732041d
msgid "centos:yum install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:67
#: 30cae887feee4ec897d787801d1900db
msgid "ubuntu:app-get install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:69
#: 292a52e7f50242c59e7e95bafc8102da
msgid "macos:brew install git-lfs"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:86
#: a66af00ea6a0403dbc90df23b0e3c40e
msgid ""
"The model files are large and will take a long time to download. During "
"the download, let's configure the .env file, which needs to be copied and"
" created from the .env.template"
msgstr "模型文件很大，需要很长时间才能下载。在下载过程中，让我们配置.env文件，它需要从。env.template中复制和创建。"

#: ../../getting_started/install/deploy/deploy.md:88
#: 98934f9d2dda41e9b9bc20078ed750eb
msgid ""
"if you want to use openai llm service, see [LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"
msgstr ""
"如果想使用openai大模型服务, 可以参考[LLM Use FAQ](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/faq/llm/llm_faq.html)"

#: ../../getting_started/install/deploy/deploy.md:91
#: 727fc8be76dc40d59416b20273faee13
msgid "cp .env.template .env"
msgstr "cp .env.template .env"

#: ../../getting_started/install/deploy/deploy.md:94
#: 65a5307640484800b2d667585e92b340
msgid ""
"You can configure basic parameters in the .env file, for example setting "
"LLM_MODEL to the model to be used"
msgstr "您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。"

#: ../../getting_started/install/deploy/deploy.md:96
#: ab83b0d6663441e588ceb52bb2e5934c
msgid ""
"([Vicuna-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5) based on "
"llama-2 has been released, we recommend you set `LLM_MODEL=vicuna-"
"13b-v1.5` to try this model)"
msgstr ""
"您可以在.env文件中配置基本参数，例如将LLM_MODEL设置为要使用的模型。([Vicuna-v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5)， "
"目前Vicuna-v1.5模型(基于llama2)已经开源了，我们推荐你使用这个模型通过设置LLM_MODEL=vicuna-13b-v1.5"

#: ../../getting_started/install/deploy/deploy.md:98
#: 4ed331ceacb84a339a7e0038029e356e
msgid "3. Run"
msgstr "3. Run"

#: ../../getting_started/install/deploy/deploy.md:100
#: e674fe7a6a9542dfae6e76f8c586cb04
msgid "**(Optional) load examples into SQLlite**"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:105
#: b2de940ecd9444d2ab9ebf8762565fb6
msgid "1.Run db-gpt server"
msgstr "1.Run db-gpt server"

#: ../../getting_started/install/deploy/deploy.md:111
#: 929d863e28eb4c5b8bd4c53956d3bc76
msgid "Open http://localhost:5000 with your browser to see the product."
msgstr "打开浏览器访问http://localhost:5000"

#: ../../getting_started/install/deploy/deploy.md:114
#: 43dd8b4a017f448f9be4f5432a083c08
msgid "If you want to access an external LLM service, you need to"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:116
#: 452a8bbc3e7f43e9a89f244ab0910fd6
msgid ""
"1.set the variables LLM_MODEL=YOUR_MODEL_NAME, "
"MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in the .env "
"file."
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:118
#: 216350f67d1a4056afb1c0277dd46a0c
msgid "2.execute dbgpt_server.py in light mode"
msgstr ""

#: ../../getting_started/install/deploy/deploy.md:121
#: fc9c90ca4d974ee1ade9e972320debba
msgid ""
"If you want to learn about dbgpt-webui, read https://github./csunny/DB-"
"GPT/tree/new-page-framework/datacenter"
msgstr ""
"如果你想了解web-ui, 请访问https://github./csunny/DB-GPT/tree/new-page-"
"framework/datacenter"

#: ../../getting_started/install/deploy/deploy.md:127
#: c0df0cdc5dea4ef3bef4d4c1f4cc52ad
#, fuzzy
msgid "Multiple GPUs"
msgstr "4. Multiple GPUs"

#: ../../getting_started/install/deploy/deploy.md:129
#: c3aed00fa8364e6eaab79a23cf649558
msgid ""
"DB-GPT will use all available gpu by default. And you can modify the "
"setting `CUDA_VISIBLE_DEVICES=0,1` in `.env` file to use the specific gpu"
" IDs."
msgstr "DB-GPT默认加载可利用的gpu，你也可以通过修改 在`.env`文件 `CUDA_VISIBLE_DEVICES=0,1`来指定gpu IDs"

#: ../../getting_started/install/deploy/deploy.md:131
#: 88b7510fef5943c5b4807bc92398a604
msgid ""
"Optionally, you can also specify the gpu ID to use before the starting "
"command, as shown below:"
msgstr "你也可以指定gpu ID启动"

#: ../../getting_started/install/deploy/deploy.md:141
#: 2cf93d3291bd4f56a3677e607f6185e7
msgid ""
"You can modify the setting `MAX_GPU_MEMORY=xxGib` in `.env` file to "
"configure the maximum memory used by each GPU."
msgstr "同时你可以通过在.env文件设置`MAX_GPU_MEMORY=xxGib`修改每个GPU的最大使用内存"

#: ../../getting_started/install/deploy/deploy.md:143
#: 00aac35bec094f99bd1f2f5f344cd3f5
#, fuzzy
msgid "Not Enough Memory"
msgstr "5. Not Enough Memory"

#: ../../getting_started/install/deploy/deploy.md:145
#: d3d4b8cd24114a24929f62bbe7bae1a2
msgid "DB-GPT supported 8-bit quantization and 4-bit quantization."
msgstr "DB-GPT 支持 8-bit quantization 和 4-bit quantization."

#: ../../getting_started/install/deploy/deploy.md:147
#: eb9412b79f044aebafd59ecf3cc4f873
msgid ""
"You can modify the setting `QUANTIZE_8bit=True` or `QUANTIZE_4bit=True` "
"in `.env` file to use quantization(8-bit quantization is enabled by "
"default)."
msgstr "你可以通过在.env文件设置`QUANTIZE_8bit=True` or `QUANTIZE_4bit=True`"

#: ../../getting_started/install/deploy/deploy.md:149
#: b27c4982ec9b4ff0992d477be1100488
msgid ""
"Llama-2-70b with 8-bit quantization can run with 80 GB of VRAM, and 4-bit"
" quantization can run with 48 GB of VRAM."
msgstr ""
"Llama-2-70b with 8-bit quantization 可以运行在80GB VRAM机器， 4-bit "
"quantization可以运行在 48 GB  VRAM"

#~ msgid ""
#~ "Notice make sure you have install "
#~ "git-lfs centos:yum install git-lfs "
#~ "ubuntu:app-get install git-lfs "
#~ "macos:brew install git-lfs"
#~ msgstr ""
#~ "注意下载模型之前确保git-lfs已经安ubuntu:app-get install "
#~ "git-lfs macos:brew install git-lfs"

#~ msgid ""
#~ "You can refer to this document to"
#~ " obtain the Vicuna weights: "
#~ "[Vicuna](https://github.com/lm-sys/FastChat/blob/main/README.md"
#~ "#model-weights) ."
#~ msgstr ""
#~ "你可以参考如何获取Vicuna weights文档[Vicuna](https://github.com/lm-"
#~ "sys/FastChat/blob/main/README.md#model-weights) ."

#~ msgid ""
#~ "If you have difficulty with this "
#~ "step, you can also directly use "
#~ "the model from [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) as "
#~ "a replacement."
#~ msgstr ""
#~ "如果觉得模型太大你也可以下载vicuna-7b [this "
#~ "link](https://huggingface.co/Tribbiani/vicuna-7b) "

#~ msgid ""
#~ "If you want to access an external"
#~ " LLM service, you need to 1.set "
#~ "the variables LLM_MODEL=YOUR_MODEL_NAME "
#~ "MODEL_SERVER=YOUR_MODEL_SERVER（eg:http://localhost:5000） in "
#~ "the .env file. 2.execute dbgpt_server.py "
#~ "in light mode"
#~ msgstr ""
#~ "如果你想访问外部的大模型服务(是通过DB-"
#~ "GPT/pilot/server/llmserver.py启动的模型服务)，1.需要在.env文件设置模型名和外部模型服务地址。2.使用light模式启动服务"

#~ msgid ""
#~ "Note: you need to install the "
#~ "latest dependencies according to "
#~ "[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)."
#~ msgstr ""
#~ "注意，需要安装[requirements.txt](https://github.com/eosphoros-ai/DB-"
#~ "GPT/blob/main/requirements.txt)涉及的所有的依赖"

