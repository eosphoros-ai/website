# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.6\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-17 19:39+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/cluster/cluster.rst:72
msgid "Cluster deployment"
msgstr "集群部署"

#: ../../getting_started/install/cluster/cluster.rst:2
#: bc5bb85c846b4ad19aeeccdd016f3ce8
#, fuzzy
msgid "LLM Deployment"
msgstr "集群部署"

#: ../../getting_started/install/cluster/cluster.rst:3
#: e1cebf0518db423fbc78e39945a423fa
msgid ""
"In the exploration and implementation of AI model applications, it can be"
" challenging to directly integrate with model services. Currently, there "
"is no established standard for deploying large models, and new models and"
" inference methods are constantly being released. As a result, a "
"significant amount of time is spent adapting to the ever-changing "
"underlying model environment. This, to some extent, hinders the "
"exploration and implementation of AI model applications."
msgstr ""
"在AIGC应用探索和生产落地中，难以避免直接与模型服务对接，但是目前大模型的推理部署目前还没有一个事实标准，不断有新的模型发布，也不断有新的训练和推理方法被提出，而我们就不得不花费相当一部分时间来适配多变的底层模型环境，而这在一定程度上制约了"
" AIGC 应用的探索和落地。"

#: ../../getting_started/install/cluster/cluster.rst:5
#: c6179ac327734b7ca7b87612988dad29
msgid ""
"We divide the deployment of large models into two layers: the model "
"inference layer and the model deployment layer. The model inference layer"
" corresponds to model inference frameworks such as vLLM, TGI, and "
"TensorRT. The model deployment layer interfaces with the inference layer "
"below and provides model serving capabilities above. We refer to this "
"layer's framework as the model deployment framework. Positioned above the"
" inference frameworks, the model deployment framework offers capabilities"
" such as multiple model instances, multiple inference frameworks, "
"multiple service protocols, multi-cloud support, automatic scaling, and "
"observability."
msgstr ""
"我们将大模型推理部署分为两层：模型推理层、模型部署层。模型推理层，对应模型推理框架 vLLM、TGI 和 TensorRT "
"等。模型部署层向下对接推理层，向上提供模型服务能力，这一层的框架我们称为模型部署框架，模型部署框架在推理框架之上，提供了多模型实例、多推理框架、多服务协议、多云、自动扩缩容和可观测性等能力。"

#: ../../getting_started/install/cluster/cluster.rst:7
#: 61bae2fc8e3347248ecf084a3977e448
msgid ""
"In order to deploy DB-GPT to multiple nodes, you can deploy a cluster. "
"The cluster architecture diagram is as follows:"
msgstr "为了能将DB-GPT部署到多个节点上，你可以部署一个集群，集群的架构图如下:"

#: ../../getting_started/install/cluster/cluster.rst:14
#: af8d74ac3c5747b3934d02200afbb4ba
msgid "Design of DB-GPT:"
msgstr "设计目标"

#: ../../getting_started/install/cluster/cluster.rst:16
#: ab9f332105ac490097501798d7b6cf15
msgid ""
"DB-GPT is designed as a llm deployment framework, taking into account the"
" above design objectives."
msgstr "支持多模型和多推理框架"

#: ../../getting_started/install/cluster/cluster.rst:18
#: 281c38e2e84940098eeeb435db6d1f05
msgid ""
"Support for llm and inference frameworks: DB-GPT supports the "
"simultaneous deployment of llm and is compatible with multiple inference "
"frameworks such as vLLM, TGI, and TensorRT."
msgstr ""
"在 DB-GPT 中，直接提供了对 FastChat、vLLM和 llama.cpp 的无缝支持，理论上它们支持模型 DB-GPT "
"都支持，如果您对推理速度和并发能力有需求，可以直接使用 vLLM，如果您希望 CPU 或者 mac 的 "
"m1/m2性能也获得不错的推理性能，可以使用 llama.cpp，此外，DB-GPT 还支持了很多代理模型（openai、azure "
"openai、google bard、文心一言、通义千问和智谱AI等）。"

#: ../../getting_started/install/cluster/cluster.rst:20
#: ec7e111f2db64c7fa926b1491020ae73
msgid ""
"Scalability and stability: DB-GPT has good scalability, allowing easy "
"addition of new models and inference frameworks. It utilizes a "
"distributed architecture and automatic scaling capabilities to handle "
"high concurrency and large-scale requests, ensuring system stability."
msgstr "良好的扩展性和稳定性”"

#: ../../getting_started/install/cluster/cluster.rst:22
#: 49566c3e708c4ef3a6135ea6245a5417
msgid ""
"Performance optimization: DB-GPT undergoes performance optimization to "
"provide fast and efficient model inference capabilities, preventing it "
"from becoming a performance bottleneck during inference."
msgstr "框架性能 “不拖后腿”"

#: ../../getting_started/install/cluster/cluster.rst:24
#: 0ae41617a7904dcfadd64ec921d3987e
msgid ""
"Management and observability capabilities: DB-GPT offers management and "
"monitoring functionalities, including model deployment and configuration "
"management, performance monitoring, and logging. It can generate reports "
"on model performance and service status to promptly identify and resolve "
"issues."
msgstr "管理与可观测性能力"

#: ../../getting_started/install/cluster/cluster.rst:26
#: 7c7c762642754c8d8e8b7d4eaad55384
msgid ""
"Lightweight: DB-GPT is designed as a lightweight framework to improve "
"deployment efficiency and save resources. It employs efficient algorithms"
" and optimization strategies to minimize resource consumption while "
"maintaining sufficient functionality and performance."
msgstr "轻量化"

#: ../../getting_started/install/cluster/cluster.rst:29
#: 32c1d24c20ed4155ad05c505a355ebaf
msgid "1.Support for multiple models and inference frameworks"
msgstr "1.支持多模型和多推理框架"

#: ../../getting_started/install/cluster/cluster.rst:30
#: b0d80a26a0d14ab4a6a82bbdc693a9cc
msgid ""
"The field of large models is evolving rapidly, with new models being "
"released and new methods being proposed for model training and inference."
" We believe that this situation will continue for some time."
msgstr "当前大模型领域发展可谓日新月异，不断有新的模型发布，在模型训练和推理方面，也不断有新的方法被提出。我们判断，这样情况还会持续一段时间。"

#: ../../getting_started/install/cluster/cluster.rst:32
#: 8b7c3830e5d64567bef9244bd0c4442d
msgid ""
"For most users exploring and implementing AI applications, this situation"
" has its pros and cons. The benefits are apparent, as it brings new "
"opportunities and advancements. However, one drawback is that users may "
"feel compelled to constantly try and explore new models and inference "
"frameworks."
msgstr ""
"大于大部分 AIGC "
"应用场景探索和落地的用户来说，这种情况有利也有弊，利无需多言，而弊端之一就在于被“牵着鼻子走”，需要不断去尝试和探索新的模型、新的推理框架。"

#: ../../getting_started/install/cluster/cluster.rst:34
#: b2f7bf8f9ef4406989a366d66e59794b
msgid ""
"In DB-GPT, seamless support is provided for FastChat, vLLM, and "
"llama.cpp. In theory, any model supported by these frameworks is also "
"supported by DB-GPT. If you have requirements for faster inference speed "
"and concurrency, you can directly use vLLM. If you want good inference "
"performance on CPU or Apple's M1/M2 chips, you can use llama.cpp. "
"Additionally, DB-GPT also supports various proxy models from OpenAI, "
"Azure OpenAI, Google BARD, Wenxin Yiyan, Tongyi Qianwen, and Zhipu AI, "
"among others."
msgstr ""
"在 DB-GPT 中，直接提供了对 FastChat、vLLM和 llama.cpp 的无缝支持，理论上它们支持模型 DB-GPT "
"都支持，如果您对推理速度和并发能力有需求，可以直接使用 vLLM，如果您希望 CPU 或者 mac 的 "
"m1/m2性能也获得不错的推理性能，可以使用 llama.cpp，此外，DB-GPT 还支持了很多代理模型（openai、azure "
"openai、google bard、文心一言、通义千问和智谱AI等）。"

#: ../../getting_started/install/cluster/cluster.rst:37
#: 9f894d801c364d58814f295222567992
msgid "2.Have good scalability and stability"
msgstr "2.扩展性和稳定性要足够好"

#: ../../getting_started/install/cluster/cluster.rst:38
#: 5423f1f5f0e94804becd3caa500b4046
msgid ""
"A comprehensive model deployment framework consists of several "
"components: the Model Worker, which directly interfaces with the "
"underlying inference frameworks; the Model Controller, which manages and "
"maintains multiple model components; and the Model API, which provides "
"external model serving capabilities."
msgstr ""
"一个比较完善模型部署框架需要多个部分组成，与底层推理框架直接对接的 Model Worker，管理和维护多个模型组件的 Model "
"Controller 以及对外提供模型服务能力的 Model API。"

#: ../../getting_started/install/cluster/cluster.rst:40
#: 6e0948e9a239405ca7d90543569f35fa
msgid ""
"The Model Worker plays a crucial role and needs to be highly extensible. "
"It can be specialized for deploying large language models, embedding "
"models, or other types of models. The choice of Model Worker depends on "
"the deployment environment, such as a regular physical server "
"environment, a Kubernetes environment, or specific cloud environments "
"provided by various cloud service providers."
msgstr ""
"其中 Model Worker 必须要可以扩展，可以是专门部署大语言模型的 Model Worker，也可以是用来部署 Embedding 模型的"
" Model Worker。"

#: ../../getting_started/install/cluster/cluster.rst:42
#: 54eba96c95c847e6af77ba94114419ab
msgid ""
"Having different Model Worker options allows users to select the most "
"suitable one based on their specific requirements and infrastructure. "
"This flexibility enables efficient deployment and utilization of models "
"across different environments."
msgstr "当然也可以根据部署的环境，如普通物理机环境、kubernetes 环境以及一些特定云服务商提供的云环境等来选择不同 Model Worker"

#: ../../getting_started/install/cluster/cluster.rst:44
#: 693ec0c1b9274f64a1d8fcbd5a8a273d
msgid ""
"The Model Controller, responsible for managing model metadata, also needs"
" to be scalable. Different deployment environments and model management "
"requirements may call for different choices of Model Controllers."
msgstr ""
"用来管理模型元数据的 Model Controller 也需要可扩展，不同的部署环境已经不同的模型管控要求来选择不同的 Model "
"Controller。"

#: ../../getting_started/install/cluster/cluster.rst:46
#: 616c0dc43dd84069bde396f1cc99e316
msgid ""
"Furthermore, I believe that model serving shares many similarities with "
"traditional microservices. In microservices, a service can have multiple "
"instances, and all instances are registered in a central registry. "
"Service consumers retrieve the list of instances based on the service "
"name from the registry and select a specific instance for invocation "
"using a load balancing strategy."
msgstr "另外，在我看来，模型服务与传统的微服务有很多共通之处，在微服务中，微服务中某个服务可以有多个服务实例，所有的服务实例都统一注册到注册中心，服务调用方根据服务名称从注册中心拉取该服务名对应的服务列表，然后根据一定的负载均衡策略选择某个具体的服务实例去调用。"

#: ../../getting_started/install/cluster/cluster.rst:48
#: 83389d65894f44598a0eda3984a41cb3
msgid ""
"Similarly, in model deployment, a model can have multiple instances, and "
"all instances can be registered in a model registry. Model service "
"consumers retrieve the list of instances based on the model name from the"
" registry and select a specific instance for invocation using a model-"
"specific load balancing strategy."
msgstr "而在模型部署中，也可以考虑这样的架构，某一个模型可以有多个模型实例，所有的模型实例都统一注册到模型注册中心，然后模型服务调用方根据模型名称到注册中心去拉取模型实例列表，然后根据模型的负载均衡策略去调用某个具体的的模型实例。"

#: ../../getting_started/install/cluster/cluster.rst:50
#: 8524b6f0536446a6900715aaefcdee98
msgid ""
"Introducing a model registry, responsible for storing model instance "
"metadata, enables such an architecture. The model registry can leverage "
"existing service registries used in microservices (such as Nacos, Eureka,"
" etcd, Consul, etc.) as implementations. This ensures high availability "
"of the entire deployment system."
msgstr ""
"这里我们引入模型注册中心，它负责存储 Model Controller 中的模型实例元数据，它可以直接使用微服务中的注册中心作为实现（如 "
"nacos、eureka、etcd 和 consul 等），这样整个部署系统便可以做到高可用。"

#: ../../getting_started/install/cluster/cluster.rst:53
#: ff904ff9192248bda12b5ccae28df26f
msgid "3.High performance for framework."
msgstr "3.框架性能“不拖后腿”"

#: ../../getting_started/install/cluster/cluster.rst:54
#: c6baf9f2a059487bbf7c3996e401effb
msgid ""
"and optimization are complex tasks, and inappropriate framework designs "
"can increase this complexity. In our view, to ensure that the deployment "
"framework does not lag behind in terms of performance, there are two main"
" areas of focus:"
msgstr "框架层不应该成为模型推理性能的瓶颈，大部分情况下，硬件及推理框架决定了模型服务的服务能力，模型的推理部署和优化是一项复杂的工程，而不恰当的框架设计却可能增加这种复杂度，在我们看来，部署框架为了在性能上“不拖后腿”，有两个主要关注点："

#: ../../getting_started/install/cluster/cluster.rst:56
#: f74418d5394b4afd96578c28ff306116
msgid ""
"Avoid excessive encapsulation: The more encapsulation and longer the "
"chain, the more challenging it becomes to identify performance issues."
msgstr "避免过多的封装：封装越多、链路越长，性能问题越难以排查。"

#: ../../getting_started/install/cluster/cluster.rst:58
#: 692bc702a7f54d48b67c36ac1dc38867
msgid ""
"High-performance communication design: High-performance communication "
"involves various aspects that cannot be elaborated in detail here. "
"However, considering that Python occupies a prominent position in current"
" AIGC applications, asynchronous interfaces are crucial for service "
"performance in Python. Therefore, the model serving layer should only "
"provide asynchronous interfaces and be compatible with the layers that "
"interface with the model inference framework. If the model inference "
"framework offers asynchronous interfaces, direct integration should be "
"implemented. Otherwise, synchronous-to-asynchronous task conversion "
"should be used to provide support."
msgstr ""
"高性能的通信设计：高性能通信涉及的点很多，这里不做过多阐述。由于目前 AIGC 应用中，Python 占据领导地位，在 Python "
"中，异步接口对于服务的性能至关重要，因此，模型服务层只提供异步接口，与模型推理框架对接的层做兼容，如果模型推理框架提供了异步接口则直接对接，否则使用同步转异步的任务的方式支持。"

#: ../../getting_started/install/cluster/cluster.rst:61
#: 3b2bed671a264a13a61b7337e4577185
msgid "4.Management and monitoring capabilities."
msgstr "4.具备一定的管理和监控能力"

#: ../../getting_started/install/cluster/cluster.rst:62
#: 010c26d97cb748d28f86d9d58bdb3c6d
msgid ""
"In the exploration or production implementation of AIGC (Artificial "
"Intelligence and General Computing) applications, it is important for the"
" model deployment system to have certain management capabilities. This "
"involves controlling the deployed model instances through APIs or "
"command-line interfaces, such as for online/offline management, "
"restarting, and debugging."
msgstr ""
"在 AIGC 应用探索中或者 AIGC 应用生产落地中，我们需要模型部署系统能具备一定管理能力：通过 API "
"或者命令行等对部署的模型实例进行一定管控（如上线、下线、重启和 debug 等）。"

#: ../../getting_started/install/cluster/cluster.rst:64
#: 49f450fdb5f24d578b4cb8427e57ec15
msgid ""
"Observability is a crucial capability in production systems, and I "
"believe it is equally, if not more, important in AIGC applications. This "
"is because user experiences and interactions with the system are more "
"complex. In addition to traditional observability metrics, we are also "
"interested in user input information and corresponding contextual "
"information, which specific model instance and parameters were invoked, "
"the content and response time of model outputs, user feedback, and more."
msgstr ""
"可观测性是生产系统的一项重要能力，个人认为在 AIGC "
"应用中，可观测性同样重要，甚至更加重要，因为用户的体验、用户与系统的交互行为更复杂，除了传统的观测指标外，我们还更加关心用户的输入信息及其对应的场景上下文信息、调用了哪个模型实例和模型参数、模型输出的内容和响应时间、用户反馈等等。"

#: ../../getting_started/install/cluster/cluster.rst:66
#: 18c940b2e65d4f57ba54b9671ac02254
msgid ""
"By analyzing this information, we can identify performance bottlenecks in"
" model services and gather user experience data (e.g., response latency, "
"problem resolution, and user satisfaction extracted from user content). "
"These insights serve as important foundations for further optimizing the "
"entire application."
msgstr "我们可以从这些信息中发现一部分模型服务的性能瓶颈，以及一部分用户体验数据（响应延迟如何？是否解决了用户的问题也及用户内容中提取出用户满意度等等），这些都是整个应用进一步优化的重要依据。"

#: ../../getting_started/install/cluster/cluster.rst:68
#: a1aa65d7b0694b75a8a298090b3cbfac
#, fuzzy
msgid ""
"On :ref:`Deploying on standalone mode <standalone-index>`. Standalone "
"Deployment."
msgstr "关于 :ref:`在本地机器上部署 <local-cluster-index>`。本地集群部署。"

#: ../../getting_started/install/cluster/cluster.rst:69
#: 2d74de97891c4a31806ce286c3818631
#, fuzzy
msgid ""
"On :ref:`Deploying on cluster mode <local-cluster-index>`. Cluster "
"Deployment."
msgstr "关于 :ref:`在本地机器上部署 <local-cluster-index>`。本地集群部署。"

#~ msgid ""
#~ "When it comes to model deployment, "
#~ "performance is of utmost importance. The"
#~ " framework should be optimized to "
#~ "ensure efficient and fast model "
#~ "inference capabilities. It should not "
#~ "become a performance bottleneck and "
#~ "should be capable of handling high "
#~ "volumes of requests without compromising "
#~ "response times."
#~ msgstr "框架层不应该成为模型推理性能的瓶颈，大部分情况下，硬件及推理框架决定了模型服务的服务能力，模型的推理部署和优化是一项复杂的工程，而不恰当的框架设计却可能增加这种复杂度，在我们看来，部署框架为了在性能上“不拖后腿”，有两个主要关注点："

#~ msgid ""
#~ "To achieve this, the framework can "
#~ "employ various performance optimization "
#~ "techniques. This may include utilizing "
#~ "efficient algorithms, leveraging hardware "
#~ "acceleration (such as GPUs or "
#~ "specialized AI chips), optimizing memory "
#~ "usage, and implementing parallel processing"
#~ " techniques to maximize throughput."
#~ msgstr ""

#~ msgid ""
#~ "By prioritizing performance optimization, the"
#~ " framework can provide seamless and "
#~ "efficient model inference, enabling real-"
#~ "time and high-performance applications "
#~ "without impeding the overall system "
#~ "performance."
#~ msgstr ""

#~ msgid ""
#~ "To ensure the stability and reliability"
#~ " of model deployment, the framework "
#~ "needs to provide management and "
#~ "monitoring functionalities. This includes "
#~ "managing the lifecycle of models, such"
#~ " as model registration, updates, and "
#~ "deletion. Additionally, the framework should"
#~ " offer monitoring and logging of "
#~ "performance metrics, resource utilization, and"
#~ " system health to promptly identify "
#~ "and resolve potential issues."
#~ msgstr ""
#~ "在 AIGC 应用探索中或者 AIGC "
#~ "应用生产落地中，我们需要模型部署系统能具备一定管理能力：通过 API "
#~ "或者命令行等对部署的模型实例进行一定管控（如上线、下线、重启和 debug 等）。"

#~ msgid ""
#~ "Management capabilities may involve user "
#~ "permission management, model versioning, and"
#~ " configuration management to facilitate "
#~ "team collaboration and manage multiple "
#~ "versions and configurations of models."
#~ msgstr ""

#~ msgid ""
#~ "Monitoring capabilities can include real-"
#~ "time monitoring of model performance "
#~ "metrics such as inference latency and"
#~ " throughput. Furthermore, monitoring system "
#~ "resource usage, such as CPU, memory, "
#~ "network, and system health, along with"
#~ " error logging, can be valuable for"
#~ " diagnostics and troubleshooting."
#~ msgstr ""
#~ "可观测性是生产系统的一项重要能力，个人认为在 AIGC "
#~ "应用中，可观测性同样重要，甚至更加重要，因为用户的体验、用户与系统的交互行为更复杂，除了传统的观测指标外，我们还更加关心用户的输入信息及其对应的场景上下文信息、调用了哪个模型实例和模型参数、模型输出的内容和响应时间、用户反馈等等。"

#~ msgid ""
#~ "By providing management and monitoring "
#~ "capabilities, the framework can assist "
#~ "users in effectively managing and "
#~ "maintaining deployed models, ensuring system"
#~ " stability and reliability, and enabling"
#~ " timely responses to and resolution "
#~ "of issues, thus enhancing overall system"
#~ " efficiency and availability."
#~ msgstr ""

