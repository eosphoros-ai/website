# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2023, csunny
# This file is distributed under the same license as the DB-GPT package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: DB-GPT 👏👏 0.3.5\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-10-07 20:28+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../getting_started/install/llm/llama/llama_cpp.md:1
#: 95a9a605d97346fb98e0c0977524d354
msgid "llama.cpp"
msgstr "llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:5
#: ebe3be273a42492d9832512554b4b7dc
msgid ""
"DB-GPT already supports "
"[llama.cpp](https://github.com/ggerganov/llama.cpp) via [llama-cpp-"
"python](https://github.com/abetlen/llama-cpp-python)."
msgstr ""
"DB-GPT已经通过[llama-cpp-python](https://github.com/abetlen/llama-cpp-"
"python)支持[llama.cpp](https://github.com/ggerganov/llama.cpp)。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:7
#: 97a4f6f95d6845258e3753803fc117a3
msgid "Running llama.cpp"
msgstr "运行 llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:9
#: 40fcdf93fe3d4542bbd84ed2d5a82623
msgid "Preparing Model Files"
msgstr "准备模型文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:11
#: f10bd034d24640d3b83572d50b2a9f71
msgid ""
"To use llama.cpp, you need to prepare a gguf format model file, and there"
" are two common ways to obtain it, you can choose either:"
msgstr "使用 llama.cpp，你需要准备 gguf 格式的文件，你可以通过以下两种方法获取"

#: ../../getting_started/install/llm/llama/llama_cpp.md:13
#: fb143586b13849f0bb2b6ae0c9408e95
msgid "Download a pre-converted model file."
msgstr "下载已转换的模型文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:15
#: a6e89c960ebd4778b8fc72d3d43e9543
msgid ""
"Suppose you want to use [Vicuna 13B v1.5](https://huggingface.co/lmsys"
"/vicuna-13b-v1.5), you can download the file already converted from "
"[TheBloke/vicuna-13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF), only one file is needed. Download it to the `models` "
"directory and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"假设您想使用[Vicuna 13B v1.5](https://huggingface.co/lmsys/vicuna-"
"13b-v1.5)您可以从[TheBloke/vicuna-"
"13B-v1.5-GGUF](https://huggingface.co/TheBloke/vicuna-"
"13B-v1.5-GGUF)下载已转换的文件，只需要一个文件。将其下载到models目录并将其重命名为 `ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:21
#: 380ebad2c5a04210a48c5d7a9913413d
msgid "Convert It Yourself"
msgstr "自行转换"

#: ../../getting_started/install/llm/llama/llama_cpp.md:23
#: cf39ca73d9c6456794fb240b164b7cbb
msgid ""
"You can convert the model file yourself according to the instructions in "
"[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run), and put the converted file in the models directory "
"and rename it to `ggml-model-q4_0.gguf`."
msgstr ""
"您可以根据[llama.cpp#prepare-data--run](https://github.com/ggerganov/llama.cpp"
"#prepare-data--run)中的说明自行转换模型文件，并把转换后的文件放在models目录中，并重命名为`ggml-"
"model-q4_0.gguf`。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:25
#: 363cbf1c0b4e4029982519238f776958
msgid "Installing Dependencies"
msgstr "安装依赖"

#: ../../getting_started/install/llm/llama/llama_cpp.md:27
#: a98c36e3d7df40f3a816c0ee451b6114
msgid ""
"llama.cpp is an optional dependency in DB-GPT, and you can manually "
"install it using the following command:"
msgstr "llama.cpp在DB-GPT中是可选安装项, 你可以通过以下命令进行安装"

#: ../../getting_started/install/llm/llama/llama_cpp.md:33
#: b0038a8ba36647c6a62eef907cb6d304
msgid "Modifying the Configuration File"
msgstr "修改配置文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:35
#: d2002da716744122a44ab4ed2e47e680
msgid "Next, you can directly modify your `.env` file to enable llama.cpp."
msgstr "修改`.env`文件使用llama.cpp"

#: ../../getting_started/install/llm/llama/llama_cpp.md:42
#: 97a5fb5d4ed649f5aa0bbb97c32d54b0
msgid ""
"Then you can run it according to [Run](https://db-"
"gpt.readthedocs.io/en/latest/getting_started/install/deploy/deploy.html#run)."
msgstr ""
"然后你可以根据[运行]"
"(https://db-gpt.readthedocs.io/projects/db-gpt-docs-zh-cn/zh_CN/latest/getting_started/install/deploy/deploy.html#run)来运行"

#: ../../getting_started/install/llm/llama/llama_cpp.md:45
#: 0e3771b6aaa141f89c813507f3317bda
msgid "More Configurations"
msgstr "更多配置文件"

#: ../../getting_started/install/llm/llama/llama_cpp.md:47
#: 0802ba524cd1458298fe6f90ae7f2da1
msgid ""
"In DB-GPT, the model configuration can be done through  `{model "
"name}_{config key}`."
msgstr "在DB-GPT中，模型配置可以通过`{模型名称}_{配置名}` 来配置。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: d461d379a523424fb5885e393498ee14
msgid "Environment Variable Key"
msgstr "环境变量键"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 0263477d0ddb4914baa0d3584b751086
msgid "default"
msgstr "默认值"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: e5188d0ded6540a0bddb46d480f8b7ac
msgid "Description"
msgstr "描述"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 213b27d0e53d4858b7576dc4f2ab4d7f
msgid "llama_cpp_prompt_template"
msgstr "llama_cpp_prompt_template"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 1cb0320826564a89a3e2f51177f8a6ed 23d93dc7d88e431ba31ff64d239a412f
#: 833d5012411a4ad58b04d50a40a29184 95aa2102191946919158ae668b2e3599
#: becdd178292a48138dcb445ba3c2a6ec
msgid "None"
msgstr "None"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: ac835806c79640aa8cd39edb11d7667c
msgid ""
"Prompt template name, now support: `zero_shot, vicuna_v1.1,alpaca,llama-2"
",baichuan-chat,internlm-chat`, If None, the prompt template is "
"automatically determined from model path。"
msgstr ""
"Prompt template 现在可以支持`zero_shot, vicuna_v1.1,alpaca,llama-2,baichuan-"
"chat,internlm-chat`, 如果是None, 可以根据模型路径来自动获取模型 Prompt template"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 41bce5a6bbf2417f8bc40e71c59405ad
msgid "llama_cpp_model_path"
msgstr "llama_cpp_model_path"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 15df4d19645b40e7a209827f9a325b8f
msgid "Model path"
msgstr "模型路径"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: caf9ddbfb787418d8b167746e3febe8c
msgid "llama_cpp_n_gpu_layers"
msgstr "llama_cpp_n_gpu_layers"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: e12e0ed2c01e4d12b41d5da533073c53
msgid "1000000000"
msgstr "1000000000"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 1f4a868d3fed4ac78bfa48e13b3a59dc
msgid ""
"Number of layers to offload to the GPU, Set this to 1000000000 to offload"
" all layers to the GPU. If your GPU VRAM is not enough, you can set a low"
" number, eg: `10`"
msgstr "要将多少网络层转移到GPU上，将其设置为1000000000以将所有层转移到GPU上。如果您的 GPU 内存不足，可以设置较低的数字，例如：10。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 306e083489e24f819d67f38e2f155f0f
msgid "llama_cpp_n_threads"
msgstr "llama_cpp_n_threads"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 0490a543f67f4ecd8588541399846951
msgid ""
"Number of threads to use. If None, the number of threads is automatically"
" determined"
msgstr "要使用的线程数量。如果为None，则线程数量将自动确定。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 2ad3f09e1f894e30ae512e1cd803af52
msgid "llama_cpp_n_batch"
msgstr "llama_cpp_n_batch"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: c495776868394df5b311087dfc7c55dd
msgid "512"
msgstr "512"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: b5e69dc488cc4ae78ee9daefcf73c290
msgid "Maximum number of prompt tokens to batch together when calling llama_eval"
msgstr "在调用llama_eval时，批处理在一起的prompt tokens的最大数量"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 516cfc3ed00c4a6181f37a4649c9f041
msgid "llama_cpp_n_gqa"
msgstr "llama_cpp_n_gqa"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 51847a305c4341af8614a2ceb7aa658f
msgid "Grouped-query attention. Must be 8 for llama-2 70b."
msgstr "对于 llama-2 70B 模型，Grouped-query attention 必须为8。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 8261108709f341dab19e4fece7682c0c
msgid "llama_cpp_rms_norm_eps"
msgstr "llama_cpp_rms_norm_eps"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 72cc3d9988414f489ddefe3afb332e83
msgid "5e-06"
msgstr "5e-06"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: ebc1baebf57e4009b0fdfa68eb055d80
msgid "5e-6 is a good value for llama-2 models."
msgstr "对于llama-2模型来说，5e-6是一个不错的值。"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 0cc1199e293741f087c795230d9c8dda
msgid "llama_cpp_cache_capacity"
msgstr "llama_cpp_cache_capacity"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 7d13612da75046b1a3fc0877e229bb91
msgid "Maximum cache capacity. Examples: 2000MiB, 2GiB"
msgstr "模型缓存最大值. 例如: 2000MiB, 2GiB"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 53332858d3a8472f8eb59d845c594ffd
msgid "llama_cpp_prefer_cpu"
msgstr "llama_cpp_prefer_cpu"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 7ff31fe3233a4243840584bc069654cd
msgid "False"
msgstr "False"

#: ../../getting_started/install/llm/llama/llama_cpp.md
#: 62d1dbd4f8254141a697448a7a5f6701
msgid ""
"If a GPU is available, it will be preferred by default, unless "
"prefer_cpu=False is configured."
msgstr "如果有可用的GPU，默认情况下会优先使用GPU，除非配置了 prefer_cpu=False。"

#: ../../getting_started/install/llm/llama/llama_cpp.md:61
#: 8de97de28d1a40c3b852a1268255ebed
msgid "GPU Acceleration"
msgstr "GPU 加速"

#: ../../getting_started/install/llm/llama/llama_cpp.md:63
#: 8bce74c0ddb5486190ff4d36fd5358be
msgid ""
"GPU acceleration is supported by default. If you encounter any issues, "
"you can uninstall the dependent packages with the following command:"
msgstr "默认情况下支持GPU加速。如果遇到任何问题，您可以使用以下命令卸载相关的依赖包"

#: ../../getting_started/install/llm/llama/llama_cpp.md:68
#: 1f3fe88521614d499cb1d046f8d3c125
msgid ""
"Then install `llama-cpp-python` according to the instructions in [llama-"
"cpp-python](https://github.com/abetlen/llama-cpp-"
"python/blob/main/README.md)."
msgstr ""
"然后通过指令[llama-cpp-python](https://github.com/abetlen/llama-cpp-"
"python/blob/main/README.md).安装`llama-cpp-python`"

#: ../../getting_started/install/llm/llama/llama_cpp.md:71
#: fc83106f0a0e4ddfb3c058bec62f4568
msgid "Mac Usage"
msgstr "Mac 使用"

#: ../../getting_started/install/llm/llama/llama_cpp.md:73
#: dcf5904a444342c8a768c4da8b777828
msgid ""
"Special attention, if you are using Apple Silicon (M1) Mac, it is highly "
"recommended to install arm64 architecture python support, for example:"
msgstr "特别注意：如果您正在使用苹果芯片（M1）的Mac电脑，强烈建议安装 arm64 架构的 Python 支持，例如："

#: ../../getting_started/install/llm/llama/llama_cpp.md:80
#: 547369c011a9412589dad1fac7ac3ef9
msgid "Windows Usage"
msgstr "Windows使用"

#: ../../getting_started/install/llm/llama/llama_cpp.md:82
#: 506fda57977f4aa8b9fe427e3c66f4d7
msgid ""
"The use under the Windows platform has not been rigorously tested and "
"verified, and you are welcome to use it. If you have any problems, you "
"can create an [issue](https://github.com/eosphoros-ai/DB-GPT/issues) or "
"[contact us](https://github.com/eosphoros-ai/DB-GPT/tree/main#contact-"
"information) directly."
msgstr ""
"在Windows平台上的使用尚未经过严格的测试和验证，欢迎您使用。如果您有任何问题，可以创建一个[issue](https://github.com"
"/eosphoros-ai/DB-GPT/issues)或者直接[联系我们](https://github.com/eosphoros-ai"
"/DB-GPT/tree/main#cntact-information)。"

#~ msgid ""
#~ "DB-GPT is now supported by "
#~ "[llama-cpp-python](https://github.com/abetlen/llama-"
#~ "cpp-python) through "
#~ "[llama.cpp](https://github.com/ggerganov/llama.cpp)."
#~ msgstr ""
#~ "DB-GPT is now supported by "
#~ "[llama-cpp-python](https://github.com/abetlen/llama-"
#~ "cpp-python) through "
#~ "[llama.cpp](https://github.com/ggerganov/llama.cpp)."

#~ msgid "Prompt Template Name"
#~ msgstr "Prompt Template Name"

